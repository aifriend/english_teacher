{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import evaluate\n", "from datasets import load_dataset\n", "from datasets import load_metric\n", "from dotenv import load_dotenv, find_dotenv\n", "from pandas import DataFrame"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from english_improvement_agent.EnglishImprovementAgentSummary import EnglishImprovementAgentSummary\n", "from english_improvement_agent.EnglishImprovementAgentWrite import EnglishImprovementAgentWrite\n", "from english_improvement_agent.commonsLib import loggerElk"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["logger = loggerElk(__name__)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_csv(df, dataset):\n", "    for case in dataset:\n", "        # Adding the task's prefix to input\n", "        input_text = case[\"sentence\"]\n", "        for correction in case[\"corrections\"]:\n", "            # a few of the cases contain blank strings.\n", "            if input_text and correction:\n", "                df.loc[len(df)] = dict(\n", "                    input=input_text, target=correction)\n", "    unique_df = df.drop_duplicates()\n", "    return unique_df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eval_blue(eval_name: str, dataset: DataFrame, model, metric):\n", "    logger.Information(f\"eval with score {eval_name} for {len(dataset)} examples\")\n", "    for index, row in dataset.iterrows():\n", "        prediction = model(row['input'])\n", "        correction = row['target']\n", "        logger.Information(f\"with prediction: '{prediction}'\")\n", "        logger.Information(f\"and correction: '{correction}'\")\n", "        metric.add_batch(predictions=[prediction], references=[correction])\n", "    eval_score = metric.compute()\n", "    return eval_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eval_write_grammar():\n", "    logger.Information(\"loading dataset: JFLEG\")\n", "    df = DataFrame(columns=[\"input\", \"target\"])\n", "    eval_dataset = load_dataset(\"jfleg\", split='test[:]')\n", "    df = get_csv(df, eval_dataset)\n", "    eval_df = df.sample(MAX_EVAL_SAMPLES)\n", "    write_service = EnglishImprovementAgentWrite()\n", "    metric = load_metric('bleurt')\n", "    score = eval_blue('vannity', eval_df, write_service.write_properly_vannity, metric)\n", "    logger.Information(\n", "        f\"BLUE score for write properly with HF Vannity model: {score}\")\n", "    score = eval_blue('coedit', eval_df, write_service.write_properly_coedit, metric)\n", "    logger.Information(\n", "        f\"BLUE score for write properly with HF Coedit model: {score}\")\n", "    score = eval_blue('instruct', eval_df, write_service.write_properly_instruct, metric)\n", "    logger.Information(\n", "        f\"BLUE score for write properly with OpenAI Instruct model: {score}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_dict(df, dataset):\n", "    for case in dataset:\n", "        # Adding the task's prefix to input\n", "        input_text = case[\"article\"]\n", "        summary = case[\"summary\"]\n", "        # a few of the cases contain blank strings.\n", "        if input_text and summary:\n", "            df.loc[len(df)] = dict(\n", "                input=input_text, target=summary)\n", "    unique_df = df.drop_duplicates()\n", "    return unique_df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eval_rouge(eval_name: str, dataset: DataFrame, model, metric):\n", "    logger.Information(f\"eval with score {eval_name} for {len(dataset)} examples\")\n", "    for index, row in dataset.iterrows():\n", "        prediction = model(row['input'])\n", "        correction = row['target']\n", "        logger.Information(f\"with prediction: '{prediction}'\")\n", "        logger.Information(f\"and summary: '{correction}'\")\n", "        metric.add_batch(predictions=[prediction], references=[correction])\n", "    eval_score = metric.compute()\n", "    return eval_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def eval_summary():\n", "    logger.Information(\"loading dataset: ROSE\")\n", "    df = DataFrame(columns=[\"input\", \"target\"])\n", "    eval_dataset = load_dataset(\n", "        'json', data_files='../data/wikisum.jsonl', split='train')\n", "    df = get_dict(df, eval_dataset)\n", "    eval_df = df.sample(MAX_EVAL_SAMPLES)\n", "    summary_service = EnglishImprovementAgentSummary()\n", "    metric = evaluate.load('rouge')\n", "    score = eval_rouge('T5', eval_df, summary_service.summarization_with_t5, metric)\n", "    logger.Information(\n", "        f\"ROUGE score for summary with HF T5 model: {score}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    MAX_EVAL_SAMPLES = 10\n\n", "    # not used in this stub but often useful for finding various files\n", "    project_dir = Path(__file__).resolve().parents[2]\n\n", "    # find .env automagically by walking up directories until it's found, then\n", "    # load up the .env entries as environment variables\n", "    load_dotenv(find_dotenv())\n", "    eval_write_grammar()\n", "    eval_summary()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}